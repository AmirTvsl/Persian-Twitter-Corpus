# Persian-Twitter-Corpus
A corpus compiler from twitter API
In order to be able to access Twitter API, getting authorization is needed and one should already have an account, then create a twitter app to be able to run twitter by a program on the user’s behalf. This passive application is available on Twitter development’s web page.

 By signing up there, an independent twitter app with the specific access token, consumer key, and secret would be created. Installing tweepy via pip install command makes it possible to be imported into python shell. The authorization to have access to the stream of data in Twitter API is now fulfilled where massive data is on stream. Then one would use the code in the repository to get desired information from the stream.

 Each tweet’s information is not just about the text itself, it contains much more information about the geographical emergence of data, personal information, account information and a lot more. So the texts contain additional tagged information just like annotated corpora and the features of Data Analysis are possible with this type of corpus. 

After the authorization, only three library dependencies are enough to be imported into python program that is tweepy, Authhandler and StreamListener. Next step would be to classify the stream listener for python and to define what is desired to be extracted from the stream. Twitter stream could be filtered by anything from splitting the result to language of the text, so the first API call was made by filtration of the date, “ایران ‘’ in ‘fa’ and splitting the results by removing other modification of data such as public personal information, number of posts, followers and many more which is tagged to the data. It is also worth mentioning that there are 150 API calls available per day for any program or bot to make for free.
